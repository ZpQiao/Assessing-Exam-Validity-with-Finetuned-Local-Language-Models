"""
æ”¯æŒå¾®è°ƒæ¨¡å‹ï¼ˆLoRAï¼‰çš„æµ‹è¯•æ¡†æ¶
åœ¨åŸæœ‰åŸºç¡€ä¸Šå¢åŠ äº† LoRA é€‚é…å™¨åŠ è½½åŠŸèƒ½
å·²æ›´æ–° prompt æ ¼å¼ä¸ºç¬¬äºŒä»½ä»£ç çš„ç®€æ´ç‰ˆæœ¬
"""

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel  # ğŸ”§ æ–°å¢ï¼šç”¨äºåŠ è½½ LoRA æ¨¡å‹
from typing import List, Dict
import pandas as pd
from tqdm import tqdm
import time
from datetime import datetime
import os
import re

class ProbabilityTestFramework:
    def __init__(self, 
                 model_name: str,
                 device: str = "cuda",
                 use_quantization: bool = False,
                 quantization_bits: int = 4,
                 local_model_path: str = None,
                 lora_adapter_path: str = None,  # ğŸ”§ æ–°å¢ï¼šLoRA é€‚é…å™¨è·¯å¾„
                 temperature: float = 0.0):
        """
        åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶
        
        Args:
            model_name: åŸºç¡€æ¨¡å‹åç§°
            device: è¿è¡Œè®¾å¤‡
            use_quantization: æ˜¯å¦ä½¿ç”¨é‡åŒ–
            quantization_bits: é‡åŒ–ä½æ•° (4 or 8)
            local_model_path: æœ¬åœ°åŸºç¡€æ¨¡å‹è·¯å¾„
            lora_adapter_path: LoRA é€‚é…å™¨è·¯å¾„ï¼ˆå¾®è°ƒè¾“å‡ºç›®å½•ï¼‰ğŸ†•
            temperature: ç”Ÿæˆæ¸©åº¦
        """
        self.temperature = temperature
        self.model_name = model_name
        self.device = device
        self.use_quantization = use_quantization
        self.lora_adapter_path = lora_adapter_path
        
        # å†³å®šåŸºç¡€æ¨¡å‹è·¯å¾„
        actual_model_path = model_name
        if local_model_path and os.path.exists(local_model_path):
            actual_model_path = local_model_path
            print(f"âœ“ åŸºç¡€æ¨¡å‹: {local_model_path}")
        else:
            print(f"âœ“ åŸºç¡€æ¨¡å‹: {model_name} (ä» HF)")
        
        # ğŸ”§ æ˜¾ç¤º LoRA ä¿¡æ¯
        if lora_adapter_path:
            if os.path.exists(lora_adapter_path):
                print(f"âœ“ LoRA é€‚é…å™¨: {lora_adapter_path}")
            else:
                print(f"âš ï¸ LoRA è·¯å¾„ä¸å­˜åœ¨: {lora_adapter_path}")
                print(f"   å°†åªåŠ è½½åŸºç¡€æ¨¡å‹")
                self.lora_adapter_path = None
        
        print(f"  é‡åŒ–: {'æ˜¯ (' + str(quantization_bits) + '-bit)' if use_quantization else 'å¦ (BF16)'}")
        print(f"  æ¸©åº¦: {temperature} ({'ç¡®å®šæ€§' if temperature == 0 else 'éšæœºæ€§'})")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            actual_model_path,
            use_fast=False,
            trust_remote_code=True
        )
        
        # é…ç½®é‡åŒ–å‚æ•°
        if use_quantization:
            if quantization_bits == 4:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
            elif quantization_bits == 8:
                quant_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–ä½æ•°: {quantization_bits}")
            
            # åŠ è½½é‡åŒ–çš„åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                actual_model_path,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                quantization_config=quant_config,
                trust_remote_code=True,
            )
        else:
            # åŠ è½½ BF16 åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                actual_model_path,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
        
        # ğŸ”§ åŠ è½½ LoRA é€‚é…å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.lora_adapter_path and os.path.exists(self.lora_adapter_path):
            print(f"ğŸ”§ åŠ è½½ LoRA é€‚é…å™¨...")
            try:
                self.model = PeftModel.from_pretrained(
                    base_model,
                    self.lora_adapter_path,
                    is_trainable=False  # æ¨ç†æ¨¡å¼
                )
                print(f"âœ“ LoRA é€‚é…å™¨åŠ è½½æˆåŠŸï¼ˆå¾®è°ƒæ¨¡å‹ï¼‰")
                self.is_finetuned = True
            except Exception as e:
                print(f"âš ï¸ LoRA åŠ è½½å¤±è´¥: {e}")
                print(f"   ä½¿ç”¨åŸºç¡€æ¨¡å‹")
                self.model = base_model
                self.is_finetuned = False
        else:
            self.model = base_model
            self.is_finetuned = False
        
        # åˆ¤æ–­æ¨¡å‹ç±»å‹
        self.is_thinking = "thinking" in model_name.lower()
        
        model_type = "å¾®è°ƒæ¨¡å‹" if self.is_finetuned else "åŸºç¡€æ¨¡å‹"
        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ! ç±»å‹: {model_type}")

    def format_question(self, item: Dict, language: str = "english") -> str:
        """
        ğŸ”§ æ›´æ–°ï¼šä½¿ç”¨ç¬¬äºŒä»½ä»£ç ä¸­çš„ç®€æ´ prompt æ ¼å¼
        æ ¼å¼åŒ–é—®é¢˜ä¸º promptï¼ˆä½¿ç”¨ JSON æ ¼å¼è¾“å‡ºï¼Œæ•°å­—ç­”æ¡ˆï¼‰
        """
        lang_data = item[language]
        
        # ä½¿ç”¨æ•°å­—æ ‡è®°é€‰é¡¹ï¼ˆ1, 2, 3, ...ï¼‰
        options_text = ""
        for i, option in enumerate(lang_data['options'], 1):
            options_text += f"{i}. {option}\n"
        
        # æ ¹æ®è¯­è¨€è°ƒæ•´ prompt
        if language == "english":
            prompt = f"""Please solve the following probability theory problem and select the correct answer.

Context: {lang_data['context']}

Question: {lang_data['question']}

Options:
{options_text}

Please respond with your reasoning followed by a JSON object in this exact format:
{{"answer": N}}

where N is the number of your chosen option (1, 2, 3, 4, 5, or 6).

Your response:"""
        else:  # danish
            prompt = f"""LÃ¸s fÃ¸lgende sandsynlighedsteori problem og vÃ¦lg det rigtige svar.

Kontekst: {lang_data['context']}

SpÃ¸rgsmÃ¥l: {lang_data['question']}

Valgmuligheder:
{options_text}

Svar venligst med din rÃ¦sonnering efterfulgt af et JSON-objekt i dette nÃ¸jagtige format:
{{"answer": N}}

hvor N er nummeret pÃ¥ dit valgte svarmulighed (1, 2, 3, 4, 5 eller 6).

Dit svar:"""
        
        return prompt

    def generate_answer(self, prompt: str, max_tokens: int = 16000) -> Dict:
        """ç”Ÿæˆç­”æ¡ˆ"""
        messages = [{"role": "user", "content": prompt}]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=self.is_thinking
        )
        
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        gen_config = {
            "max_new_tokens": max_tokens,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "temperature": self.temperature,
            "do_sample": True if self.temperature > 0 else False,
        }
        
        start_time = time.time()
        try:
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **gen_config)
        except Exception as e:
            generation_time = time.time() - start_time
            return {
                'full_response': f'ERROR: {e}',
                'predicted_answer': None,
                'generation_time': generation_time,
                'has_json': False
            }
        
        generation_time = time.time() - start_time
        
        try:
            full_resp = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )
        except:
            full_resp = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return self._parse_response(full_resp, generation_time)

    def _parse_response(self, response: str, gen_time: float) -> Dict:
        """è§£æå“åº”ï¼Œæå–ç­”æ¡ˆï¼ˆä¼˜å…ˆ JSON æ ¼å¼ï¼‰"""
        result = {
            'full_response': response,
            'predicted_answer': None,
            'generation_time': gen_time,
            'has_json': False
        }
        
        # æ–¹æ³•1: JSON æ ¼å¼ï¼ˆæœ€ä¼˜å…ˆï¼Œæœ€å¯é ï¼‰
        json_patterns = [
            r'\{["\']?answer["\']?\s*:\s*(\d+)\s*\}',           # {"answer": 5}
            r'\{["\']?answer["\']?\s*:\s*["\'](\d+)["\']\s*\}', # {"answer": "5"}
            r'["\']?answer["\']?\s*[:=]\s*(\d+)',                # answer: 5
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            if matches:
                try:
                    answer = int(matches[-1])
                    if 1 <= answer <= 6:
                        result['predicted_answer'] = answer
                        result['has_json'] = True
                        return result
                except:
                    continue
        
        # æ–¹æ³•2: æ˜ç¡®ç­”æ¡ˆå£°æ˜
        answer_patterns = [
            r'(?:final answer|my answer|the answer|answer)["\']?\s*(?:is|:)\s*["\']?(?:option\s*)?(\d+)["\']?',
            r'(?:option|choice)["\']?\s*["\']?(\d+)["\']?(?:\s+is correct|\s+is the answer)',
            r'(?:I choose|I select|select|choose)["\']?\s*(?:option|choice)?\s*["\']?(\d+)["\']?',
            r'correct answer.*?(?:is|:)\s*["\']?(?:option\s*)?(\d+)["\']?',
            r'therefore.*?answer.*?(?:is|:)\s*["\']?(\d+)["\']?',
        ]
        
        search_text = response[-200:] if len(response) > 200 else response
        
        for pattern in answer_patterns:
            matches = re.findall(pattern, search_text, re.IGNORECASE)
            if matches:
                try:
                    answer = int(matches[-1])
                    if 1 <= answer <= 6:
                        result['predicted_answer'] = answer
                        return result
                except:
                    continue
        
        # æ–¹æ³•3: æœ€åå°è¯•æŸ¥æ‰¾æ•°å­—
        last_number = re.findall(r'\b([1-6])\b', response[-100:] if len(response) > 100 else response)
        if last_number:
            try:
                result['predicted_answer'] = int(last_number[-1])
            except:
                pass
        
        return result

    def test_single_item(self, item: Dict, language: str = "english") -> Dict:
        """æµ‹è¯•å•ä¸ªé¢˜ç›®"""
        prompt = self.format_question(item, language)
        result = self.generate_answer(prompt)
        
        correct_answer = int(item['answer_index'])
        predicted_answer = result['predicted_answer']
        is_correct = predicted_answer == correct_answer if predicted_answer else False
        
        return {
            'base_key': item['base_key'],
            'language': language,
            'correct_answer': correct_answer,
            'predicted_answer': predicted_answer,
            'is_correct': is_correct,
            'full_response': result['full_response'][:500],  # æˆªæ–­ä¿å­˜
            'has_json': result['has_json'],
            'generation_time': result['generation_time'],
            'model_type': 'finetuned' if self.is_finetuned else 'baseline'  # ğŸ”§ æ ‡è®°æ¨¡å‹ç±»å‹
        }

    def test_dataset(self, 
                    data: List[Dict], 
                    languages: List[str] = ['english'],
                    limit: int = None) -> pd.DataFrame:
        """åœ¨æ•°æ®é›†ä¸Šè¿è¡Œæµ‹è¯•"""
        if limit:
            data = data[:limit]
        
        results = []
        total = len(data) * len(languages)
        
        print(f"\nå¼€å§‹æµ‹è¯• (å…± {total} é¢˜)")
        
        with tqdm(total=total, desc="æµ‹è¯•è¿›åº¦") as pbar:
            for item in data:
                for lang in languages:
                    try:
                        result = self.test_single_item(item, lang)
                        results.append(result)
                    except Exception as e:
                        print(f"\né”™è¯¯: {item['base_key']}_{lang}: {e}")
                        results.append({
                            'base_key': item['base_key'],
                            'language': lang,
                            'is_correct': False,
                            'full_response': f"ERROR: {e}",
                            'model_type': 'finetuned' if self.is_finetuned else 'baseline'
                        })
                    pbar.update(1)
        
        df = pd.DataFrame(results)
        
        # æ‰“å°ç»Ÿè®¡
        self._print_statistics(df)
        
        return df

    def _print_statistics(self, df: pd.DataFrame):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 80)
        print(f"æµ‹è¯•ç»Ÿè®¡ ({'å¾®è°ƒæ¨¡å‹' if self.is_finetuned else 'åŸºç¡€æ¨¡å‹'})")
        print("=" * 80)
        print(f"æ€»é¢˜æ•°: {len(df)}")
        print(f"æ­£ç¡®æ•°: {df['is_correct'].sum()}")
        print(f"å‡†ç¡®ç‡: {df['is_correct'].mean()*100:.2f}%")
        print(f"æˆåŠŸæå–ç­”æ¡ˆ: {df['predicted_answer'].notna().sum()}/{len(df)}")
        print(f"ä½¿ç”¨ JSON æ ¼å¼: {df['has_json'].sum()}/{len(df)}")
        print(f"å¹³å‡è€—æ—¶: {df['generation_time'].mean():.2f} ç§’")
        print("=" * 80)


def load_jsonl_data(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    print(f"âœ“ åŠ è½½ {len(data)} é“é¢˜ç›®")
    return data


def compare_baseline_vs_finetuned(
    test_data_path: str,
    base_model_path: str,
    lora_adapter_path: str,
    languages: List[str] = ['english'],
    limit: int = None,
    output_dir: str = './comparison_results'
):
    """
    å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹
    
    Args:
        test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_adapter_path: LoRA é€‚é…å™¨è·¯å¾„
        languages: æµ‹è¯•è¯­è¨€
        limit: é™åˆ¶æµ‹è¯•æ•°é‡
        output_dir: ç»“æœä¿å­˜ç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("="*80)
    print("åŠ è½½æµ‹è¯•æ•°æ®")
    print("="*80)
    test_data = load_jsonl_data(test_data_path)
    
    # ========================================
    # 1. æµ‹è¯•åŸºç¡€æ¨¡å‹
    # ========================================
    # print("\n" + "="*80)
    # print("æµ‹è¯•åŸºç¡€æ¨¡å‹")
    # print("="*80)
    
    # baseline_tester = ProbabilityTestFramework(
    #     model_name="Qwen/Qwen3-14B",
    #     local_model_path=base_model_path,
    #     use_quantization=True,
    #     quantization_bits=4,
    #     lora_adapter_path=None,  # ä¸åŠ è½½ LoRA
    #     temperature=0.0
    # )
    
    # baseline_results = baseline_tester.test_dataset(
    #     test_data,
    #     languages=languages,
    #     limit=limit
    # )
    
    # # ä¿å­˜åŸºç¡€æ¨¡å‹ç»“æœ
    # baseline_file = f"{output_dir}/baseline_{timestamp}.csv"
    # baseline_results.to_csv(baseline_file, index=False, encoding='utf-8-sig')
    # print(f"\nâœ“ åŸºç¡€æ¨¡å‹ç»“æœ: {baseline_file}")
    
    # # æ¸…ç†æ˜¾å­˜
    # del baseline_tester
    # torch.cuda.empty_cache()
    
    # ========================================
    # 2. æµ‹è¯•å¾®è°ƒæ¨¡å‹
    # ========================================
    print("\n" + "="*80)
    print("æµ‹è¯•å¾®è°ƒæ¨¡å‹")
    print("="*80)
    
    finetuned_tester = ProbabilityTestFramework(
        model_name="Qwen/Qwen3-14B",
        local_model_path=base_model_path,
        use_quantization=True,
        quantization_bits=4,
        lora_adapter_path=lora_adapter_path,  # åŠ è½½ LoRA
        temperature=0.0
    )
    
    finetuned_results = finetuned_tester.test_dataset(
        test_data,
        languages=languages,
        limit=limit
    )
    
    # ä¿å­˜å¾®è°ƒæ¨¡å‹ç»“æœ
    finetuned_file = f"{output_dir}/finetuned_{timestamp}.csv"
    finetuned_results.to_csv(finetuned_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ å¾®è°ƒæ¨¡å‹ç»“æœ: {finetuned_file}")
    
    del finetuned_tester
    torch.cuda.empty_cache()
    
    # ========================================
    # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    # ========================================
    print("\n" + "="*80)
    print("å¯¹æ¯”æŠ¥å‘Š")
    print("="*80)
    
    comparison = pd.DataFrame([
        {
            'æ¨¡å‹': 'åŸºç¡€æ¨¡å‹ (Qwen3-14B)',
            'å‡†ç¡®ç‡': f"{baseline_results['is_correct'].mean()*100:.2f}%",
            'æ­£ç¡®æ•°': baseline_results['is_correct'].sum(),
            'æ€»é¢˜æ•°': len(baseline_results),
            'æå–æˆåŠŸç‡': f"{baseline_results['predicted_answer'].notna().mean()*100:.2f}%",
            'JSON æ ¼å¼ç‡': f"{baseline_results['has_json'].mean()*100:.2f}%"
        },
        {
            'æ¨¡å‹': 'å¾®è°ƒæ¨¡å‹ (LoRA)',
            'å‡†ç¡®ç‡': f"{finetuned_results['is_correct'].mean()*100:.2f}%",
            'æ­£ç¡®æ•°': finetuned_results['is_correct'].sum(),
            'æ€»é¢˜æ•°': len(finetuned_results),
            'æå–æˆåŠŸç‡': f"{finetuned_results['predicted_answer'].notna().mean()*100:.2f}%",
            'JSON æ ¼å¼ç‡': f"{finetuned_results['has_json'].mean()*100:.2f}%"
        }
    ])
    
    print("\n", comparison.to_string(index=False))
    
    # è®¡ç®—æå‡
    baseline_acc = baseline_results['is_correct'].mean()
    finetuned_acc = finetuned_results['is_correct'].mean()
    improvement = (finetuned_acc - baseline_acc) * 100
    
    print(f"\nğŸ“ˆ å‡†ç¡®ç‡æå‡: {improvement:+.2f} ä¸ªç™¾åˆ†ç‚¹")
    
    if improvement > 0:
        print(f"âœ… å¾®è°ƒæœ‰æ•ˆï¼å‡†ç¡®ç‡ä» {baseline_acc*100:.2f}% æå‡åˆ° {finetuned_acc*100:.2f}%")
    elif improvement < 0:
        print(f"âš ï¸ å¾®è°ƒåå‡†ç¡®ç‡ä¸‹é™ {abs(improvement):.2f} ä¸ªç™¾åˆ†ç‚¹")
    else:
        print("å‡†ç¡®ç‡æ— å˜åŒ–")
    
    # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
    report_file = f"{output_dir}/comparison_report_{timestamp}.csv"
    comparison.to_csv(report_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ å¯¹æ¯”æŠ¥å‘Š: {report_file}")
    
    print("\n" + "="*80)
    print("å¯¹æ¯”å®Œæˆï¼")
    print("="*80)
    
    return baseline_results, finetuned_results


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

if __name__ == "__main__":
    print("="*80)
    print("æ¦‚ç‡è®ºæ¨¡å‹æµ‹è¯• - æ”¯æŒå¾®è°ƒæ¨¡å‹å¯¹æ¯”")
    print("ğŸ”§ å·²æ›´æ–°ï¼šä½¿ç”¨ç®€æ´ prompt æ ¼å¼ï¼ˆJSON è¾“å‡ºï¼‰")
    print("="*80)
    
    # é…ç½®
    TEST_DATA = "probability_test_set.jsonl"  # æµ‹è¯•é›†
    BASE_MODEL = "/root/models/qwen3-14b-q4"  # åŸºç¡€æ¨¡å‹
    LORA_ADAPTER = "./qwen3-qlora-output/final_model"  # å¾®è°ƒåçš„ LoRA
    
    TEST_LANGUAGES = ['english', 'danish']  # æˆ– ['english', 'danish']
    TEST_LIMIT = None  # None = å…¨éƒ¨ï¼Œ10 = å¿«é€Ÿæµ‹è¯•
    
    # è¿è¡Œå¯¹æ¯”
    baseline_df, finetuned_df = compare_baseline_vs_finetuned(
        test_data_path=TEST_DATA,
        base_model_path=BASE_MODEL,
        lora_adapter_path=LORA_ADAPTER,
        languages=TEST_LANGUAGES,
        limit=TEST_LIMIT,
        output_dir='./comparison_results'
    )
